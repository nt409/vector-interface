#!/bin/bash
#! Make sure all the SBATCH directives go in this section and no other commands
#! Name of the job:
#SBATCH -J VectorScan
#! Project name for Cunniffe group, use SL2 for paying queue:
#SBATCH -A CUNNIFFE-SL3-CPU
#! Output filename:
#! %A means slurm job ID and %a means array index
#SBATCH --output=%A_%a.out 
#! Errors filename:
#SBATCH --error=%A_%a.err 
#! How many whole nodes should be allocated? (for single core jobs always leave this at 1)
#SBATCH --nodes=1
#! How many tasks will there be in total? (for single core jobs always leave this at 1)
#SBATCH --ntasks=1
#! How many many cores will be allocated per task? (for single core jobs always leave this at 1)
#SBATCH --cpus-per-task=1 
#! Estimated runtime: hh:mm:ss (job is force-stopped after if exceeded):
#! less than 12 hours on SL3 (non-paid)
#SBATCH --time=9:59:00
#! Estimated maximum memory needed (job is force-stopped if exceeded):
#! RAM is allocated in ~3420mb blocks on cclake (5980 on skylake), you are charged per block used, 
#! and unused fractions of blocks will not be usable by others.
#SBATCH --mem=3420mb
#! Submit a job array with index values between 0 and 31
#! NOTE: This must be a range, not a single number (i.e. specifying '32' here would only run one job, with index 32).
#SBATCH --array=0-31

#! This is the partition name. This will request for a node with memGB RAM for each task
#SBATCH -p skylake,cclake

# mail alert at start, end and abortion of execution
#SBATCH --mail-type=ALL

#! Don't put any #SBATCH directives below this line, it is now safe to put normal commands below this line

#! Modify the environment seen by the application. For this example we need the default modules.
. /etc/profile.d/modules.sh                # This line enables the module command
# module purge                               # Removes all modules still loaded
# module load rhel7/default-peta4            # REQUIRED - loads the basic environment
# module load use.own                        # This line loads the own module list
module load /rds/project/cag1/rds-cag1-general/epidem-modules/epidem.modules   # This line loads the Epidemiology group module list

# setup conda env
module load miniconda3-4.5.4-gcc-5.4.0-hivczbz
source activate /home/nt409/software/conda_envs/vector_env

#! Command line that we want to run:
#! The variable $SLURM_ARRAY_TASK_ID contains the array index for each job.
#! In this example, each job will be passed its index

python -m random_scan.run.find_NPT_4_eq $SLURM_ARRAY_TASK_ID
